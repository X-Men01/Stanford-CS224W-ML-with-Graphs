{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuXWJLEm2UWS"
   },
   "source": [
    "# **CS224W - Colab 2**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/scpd-proed/XCS224W-Colab2/blob/main/Notebook/XCS224W_Colab2.ipynb)\n",
    "\n",
    "Before opening the colab with the badge, you would need to allow Google Colab to access the GitHub private repositories. Please check therefore [this tutorial](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb#:~:text=Navigate%20to%20http%3A%2F%2Fcolab,to%20read%20the%20private%20files.).\n",
    "\n",
    "If colab is opened with this badge, make sure please **save copy to drive** in 'File' menu before running the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gzsP50bF6Gb"
   },
   "source": [
    "In Colab 2, you will construct your first graph neural network using PyTorch Geometric (PyG) and apply the model on two Open Graph Benchmark (OGB) datasets. These two datasets will be used to benchmark your model's performance on two different graph-based tasks: 1) node property prediction (predicting the properties of single nodes) and 2) graph property prediction (predicting properties of entire graphs or subgraphs).\n",
    "\n",
    "First, you will learn how PyTorch Geometric stores graphs as PyTorch tensors.\n",
    "\n",
    "Then, you will load and inspect one of the Open Graph Benchmark (OGB) datasets by using the `ogb` package. OGB is a collection of realistic, large-scale, and diverse benchmark datasets for machine learning on graphs. The `ogb` package not only provides data loaders for each dataset but also model evaluators.\n",
    "\n",
    "Lastly, you will build our own graph neural network using PyTorch Geometric. You will train and evaluate you model on the OGB node property prediction and graph property prediction tasks.\n",
    "\n",
    "**Note**: Make sure to **sequentially run all the cells in each section**, so that the intermediate variables / packages will carry over to the next cell\n",
    "\n",
    "Have fun and good luck on Colab 2 :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEBZncyfK80M"
   },
   "source": [
    "## Building + Debugging Notes\n",
    "While working through this Colab and future Colabs, we strongly encourage you to follow a couple of building / debugging strategies:\n",
    "- During debugging make sure to run your notebook using the CPU runtime. You can change the notebook runtime by selecting `Runtime` and then `Change runtime type`. From the dropdown, select `None` as the `hardware accelerator`.\n",
    "- When working with PyTorch and Neural Network models, understanding the shapes of different tensors, especially the input and output tensors is incredibly helpful.\n",
    "- When training models, it is helpful to start by only running 1 epoch or even just a couple of batch iterations. This way you can check that all your tensor shapes and logic match up, while also tracking expected behavior, such as a decreasing training loss. Remember to comment out / save the default number of epochs that we provide you.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGKqVEbbMEzf"
   },
   "source": [
    "# Device\n",
    "For the final testing of your models you will want to use a GPU for this Colab to run quickly.\n",
    "\n",
    "Please click `Runtime` and then `Change runtime type`. Then set the `hardware accelerator` to **GPU**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OCK7krJdp4o8"
   },
   "source": [
    "# Setup\n",
    "As discussed in Colab 0 and 1, the installation of PyG on Colab can be a little bit tricky. First let us check which version of PyTorch you are running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2vkP8pA1qBE5",
    "outputId": "22f65529-b7eb-4c47-a3b3-7525d891c4a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch has version 2.4.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "print(\"PyTorch has version {}\".format(torch.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6d22O6DqGSZ"
   },
   "source": [
    "Download the necessary packages for PyG. Make sure that your version of torch matches the output from the cell above. In case of any issues, more information can be found on the [PyG's installation page](https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "zr8hfxJ-qRg2",
    "outputId": "2f69b389-1c94-4650-83ae-75b930000877"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ogb in /opt/anaconda3/lib/python3.12/site-packages (1.3.6)\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from ogb) (2.4.0)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /opt/anaconda3/lib/python3.12/site-packages (from ogb) (1.26.4)\n",
      "Requirement already satisfied: tqdm>=4.29.0 in /opt/anaconda3/lib/python3.12/site-packages (from ogb) (4.66.4)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /opt/anaconda3/lib/python3.12/site-packages (from ogb) (1.4.2)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /opt/anaconda3/lib/python3.12/site-packages (from ogb) (2.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from ogb) (1.16.0)\n",
      "Requirement already satisfied: urllib3>=1.24.0 in /opt/anaconda3/lib/python3.12/site-packages (from ogb) (2.2.2)\n",
      "Requirement already satisfied: outdated>=0.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from ogb) (0.2.2)\n",
      "Requirement already satisfied: setuptools>=44 in /opt/anaconda3/lib/python3.12/site-packages (from outdated>=0.2.0->ogb) (69.5.1)\n",
      "Requirement already satisfied: littleutils in /opt/anaconda3/lib/python3.12/site-packages (from outdated>=0.2.0->ogb) (0.2.4)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from outdated>=0.2.0->ogb) (2.32.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=0.24.0->ogb) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=0.24.0->ogb) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=0.24.0->ogb) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn>=0.20.0->ogb) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn>=0.20.0->ogb) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn>=0.20.0->ogb) (2.2.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.6.0->ogb) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.6.0->ogb) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.6.0->ogb) (1.13.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.6.0->ogb) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.6.0->ogb) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.6.0->ogb) (2024.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.6.0->ogb) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->outdated>=0.2.0->ogb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->outdated>=0.2.0->ogb) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->outdated>=0.2.0->ogb) (2024.6.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Install torch geometric\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "#   !pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-2.4.0+cu121.html\n",
    "#   !pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-2.4.0+cu121.html\n",
    "#   !pip install torch-geometric\n",
    "  !pip install ogb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch_geometric\n",
    "torch_geometric.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nwwq0nSdmsOL"
   },
   "source": [
    "# 1) PyTorch Geometric (Datasets and Data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sf7vUmdNKCjA"
   },
   "source": [
    "PyTorch Geometric has two classes for storing and/or transforming graphs into tensor format. One is `torch_geometric.datasets`, which contains a variety of common graph datasets. Another is `torch_geometric.data`, the class which provides the data handling of graphs as PyTorch tensors.\n",
    "\n",
    "In this section, you will learn how to use `torch_geometric.datasets` and `torch_geometric.data` together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ic-o1P3r6hr2"
   },
   "source": [
    "## PyG Datasets\n",
    "\n",
    "The `torch_geometric.datasets` class has many common graph datasets. Here you will explore its usage through one example dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zT5qca3x6XpG",
    "outputId": "08275f59-df45-434e-a558-d25ec7cb8279"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://www.chrsmrrs.com/graphkerneldatasets/ENZYMES.zip\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENZYMES(600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "import networkx as nx\n",
    "\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  root = './enzymes'\n",
    "  name = 'ENZYMES'\n",
    "\n",
    "  # The ENZYMES dataset\n",
    "  pyg_dataset= TUDataset(root, name)\n",
    "\n",
    "  # You will find that there are 600 graphs in this dataset\n",
    "  print(pyg_dataset)\n",
    "  # help(pyg_dataset)\n",
    "  # nx.draw(pyg_dataset.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLm5vVYMAP2x"
   },
   "source": [
    "## Question 1: How many classes and features are in the ENZYMES dataset? (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106\n",
      "106\n"
     ]
    }
   ],
   "source": [
    "print(pyg_dataset[200].edge_index.shape[1])\n",
    "print(len(pyg_dataset[200].edge_index[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8iF_Kyqr_JbY",
    "outputId": "e3ccf788-e912-4056-9810-e64b571cee1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENZYMES dataset has 6 classes\n",
      "ENZYMES dataset has 3 features\n"
     ]
    }
   ],
   "source": [
    "def get_num_classes(pyg_dataset):\n",
    "  # TODO: Implement a function that takes a PyG dataset object\n",
    "  # and returns the number of classes for that dataset.\n",
    "\n",
    "  num_classes = 0\n",
    "\n",
    "  ############# Your code here ############\n",
    "  ## (~1 line of code)\n",
    "  ## Note\n",
    "  ## 1. Colab autocomplete functionality might be useful.\n",
    "  num_classes = pyg_dataset.num_classes\n",
    "  #########################################\n",
    "\n",
    "  return num_classes\n",
    "\n",
    "def get_num_features(pyg_dataset):\n",
    "  # TODO: Implement a function that takes a PyG dataset object\n",
    "  # and returns the number of features for that dataset.\n",
    "\n",
    "  num_features = 0\n",
    "\n",
    "  ############# Your code here ############\n",
    "  ## (~1 line of code)\n",
    "  ## Note\n",
    "  ## 1. Colab autocomplete functionality might be useful.\n",
    "  num_features = pyg_dataset.num_features\n",
    "  #########################################\n",
    "\n",
    "  return num_features\n",
    "\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  num_classes = get_num_classes(pyg_dataset)\n",
    "  num_features = get_num_features(pyg_dataset)\n",
    "  print(\"{} dataset has {} classes\".format(name, num_classes))\n",
    "  print(\"{} dataset has {} features\".format(name, num_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwKbzhHUAckZ"
   },
   "source": [
    "## PyG Data\n",
    "\n",
    "Each PyG dataset stores a list of `torch_geometric.data.Data` objects, where each `torch_geometric.data.Data` object represents a graph. You can easily get the `Data` object by indexing into the dataset.\n",
    "\n",
    "For more information such as what is stored in the `Data` object, please refer to the [documentation](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.Data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sCV3xJWCddX"
   },
   "source": [
    "## Question 2: What is the label of the graph with index 100 in the ENZYMES dataset? (1 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LIis9oTZAfs3",
    "outputId": "8a2c0147-1b61-4601-d5c9-b55b08ba8392"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 168], x=[37, 3], y=[1])\n",
      "Graph with index 100 has label 4\n"
     ]
    }
   ],
   "source": [
    "def get_graph_class(pyg_dataset, idx):\n",
    "  # TODO: Implement a function that takes as input a PyG dataset \n",
    "  # object and the index of a graph within the dataset,\n",
    "  # and returns the class/label of the graph (as an integer).\n",
    "\n",
    "  label = -1\n",
    "\n",
    "  ############# Your code here ############\n",
    "  ## (~1 line of code)\n",
    "  # label = \n",
    "  #########################################\n",
    "  label = pyg_dataset[idx].y\n",
    "  return label.item()\n",
    "\n",
    "# Here pyg_dataset is a dataset for graph classification\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  graph_0 = pyg_dataset[0]\n",
    "  print(graph_0)\n",
    "  idx = 100\n",
    "  label = get_graph_class(pyg_dataset, idx)\n",
    "  print('Graph with index {} has label {}'.format(idx, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKhcVeAhCwoY"
   },
   "source": [
    "## Question 3: How many edges does the graph with index 200 have? (1 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f5m2DOfhBtWv",
    "outputId": "f79564ca-c0f1-4969-db86-8a10bfed653c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with index 200 has 53 edges\n"
     ]
    }
   ],
   "source": [
    "def get_graph_num_edges(pyg_dataset, idx):\n",
    "  # TODO: Implement a function that takes as inpute a PyG dataset object\n",
    "  # and the index of a graph in the dataset, and returns the number of \n",
    "  # edges in the graph (as an integer). You should not count an edge \n",
    "  # twice if the graph is undirected. For example, in an undirected \n",
    "  # graph G, if two nodes v and u are connected by an edge, this edge\n",
    "  # should only be counted once.\n",
    "\n",
    "  num_edges = 0\n",
    "\n",
    "  ############# Your code here ############\n",
    "  ## Note:\n",
    "  ## 1. You shouldn't return the data.num_edges directly\n",
    "  ## 2. We assume the graph is undirected\n",
    "  ## 3. Look at the PyG dataset built in functions\n",
    "  ## (~4 lines of code)\n",
    "    #########################################\n",
    "  num_edges = pyg_dataset[idx].num_edges\n",
    "  \n",
    "  if pyg_dataset[idx].is_undirected():\n",
    "    num_edges = num_edges // 2\n",
    "\n",
    "  \n",
    "  return num_edges\n",
    "\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  idx = 200\n",
    "  num_edges = get_graph_num_edges(pyg_dataset, idx)\n",
    "  print('Graph with index {} has {} edges'.format(idx, num_edges))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXa7yIG4E0Fp"
   },
   "source": [
    "# 2) Open Graph Benchmark (OGB)\n",
    "\n",
    "The Open Graph Benchmark (OGB) is a collection of realistic, large-scale, and diverse benchmark datasets for machine learning on graphs. Its datasets are automatically downloaded, processed, and split using the OGB Data Loader. A model's performance over these datasets can then be evaluated using the OGB Evaluator in a unified manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnazPGGAJAZN"
   },
   "source": [
    "## Dataset and Data\n",
    "\n",
    "OGB also supports PyG dataset and data classes. Here you will explore the `ogbn-arxiv` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gpc6bTm3GF02",
    "outputId": "676a6a12-9e62-429f-a74e-fa2aa001d212"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloaded 0.00 GB:   2%|‚ñè         | 2/81 [00:29<19:22, 14.72s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Stopped downloading due to interruption.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/ogb/utils/url.py:62\u001b[0m, in \u001b[0;36mdownload_url\u001b[0;34m(url, folder, log)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m---> 62\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mread(chunk_size)\n\u001b[1;32m     63\u001b[0m     downloaded_size \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/http/client.py:479\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    478\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 479\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mogbn-arxiv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load the dataset with no transformation of the adjacency matrix\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m dataset \u001b[38;5;241m=\u001b[39m PygNodePropPredDataset(name\u001b[38;5;241m=\u001b[39mdataset_name, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m dataset has \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m graph\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(dataset_name, \u001b[38;5;28mlen\u001b[39m(dataset)))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Extract the graph\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/ogb/nodeproppred/dataset_pyg.py:68\u001b[0m, in \u001b[0;36mPygNodePropPredDataset.__init__\u001b[0;34m(self, name, root, transform, pre_transform, meta_dict)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_hetero \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis hetero\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28msuper\u001b[39m(PygNodePropPredDataset, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, transform, pre_transform)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_paths[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch_geometric/data/in_memory_dataset.py:81\u001b[0m, in \u001b[0;36mInMemoryDataset.__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter, log, force_reload)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     74\u001b[0m     root: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     force_reload: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     80\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform, pre_transform, pre_filter, log,\n\u001b[1;32m     82\u001b[0m                      force_reload)\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data: Optional[BaseData] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslices: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Tensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch_geometric/data/dataset.py:112\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter, log, force_reload)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforce_reload \u001b[38;5;241m=\u001b[39m force_reload\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_download:\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download()\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_process:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch_geometric/data/dataset.py:229\u001b[0m, in \u001b[0;36mDataset._download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    228\u001b[0m fs\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 229\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/ogb/nodeproppred/dataset_pyg.py:126\u001b[0m, in \u001b[0;36mPygNodePropPredDataset.download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m url \u001b[38;5;241m=\u001b[39m  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decide_download(url):\n\u001b[0;32m--> 126\u001b[0m     path \u001b[38;5;241m=\u001b[39m download_url(url, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_root)\n\u001b[1;32m    127\u001b[0m     extract_zip(path, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_root)\n\u001b[1;32m    128\u001b[0m     os\u001b[38;5;241m.\u001b[39munlink(path)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/ogb/utils/url.py:69\u001b[0m, in \u001b[0;36mdownload_url\u001b[0;34m(url, folder, log)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(path):\n\u001b[1;32m     68\u001b[0m          os\u001b[38;5;241m.\u001b[39mremove(path)\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStopped downloading due to interruption.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m path\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Stopped downloading due to interruption."
     ]
    }
   ],
   "source": [
    "import torch_geometric.transforms as T\n",
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  dataset_name = 'ogbn-arxiv'\n",
    "  # Load the dataset with no transformation of the adjacency matrix\n",
    "  dataset = PygNodePropPredDataset(name=dataset_name, transform=None)\n",
    "  print('The {} dataset has {} graph'.format(dataset_name, len(dataset)))\n",
    "\n",
    "  # Extract the graph\n",
    "  data = dataset[0]\n",
    "  print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cw0xZJKZI-n3"
   },
   "source": [
    "## Question 4: How many features are in the ogbn-arxiv graph? (1 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZP844_nT2ZJl",
    "outputId": "9e638afd-6ec0-4c49-f46f-22628153d7d1"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m num_features\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIS_GRADESCOPE_ENV\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[0;32m---> 15\u001b[0m   num_features \u001b[38;5;241m=\u001b[39m graph_num_features(data)\n\u001b[1;32m     16\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe graph has \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m features\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(num_features))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "def graph_num_features(data):\n",
    "  # TODO: Implement a function that takes a PyG data object,\n",
    "  # and returns the number of features in the graph (as an integer).\n",
    "\n",
    "  num_features = 0\n",
    "\n",
    "  ############# Your code here ############\n",
    "  ## (~1 line of code)\n",
    "  num_features = data.num_features\n",
    "  #########################################\n",
    "\n",
    "  return num_features\n",
    "\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  num_features = graph_num_features(data)\n",
    "  print('The graph has {} features'.format(num_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DP_yEQZ0NVW"
   },
   "source": [
    "# 3) GNN: Node Property Prediction\n",
    "\n",
    "In this section you will build your first graph neural network using PyTorch Geometric. Then you will apply it to the task of node property prediction (node classification).\n",
    "\n",
    "Specifically, you will use GCN as the foundation for your graph neural network ([Kipf et al. (2017)](https://arxiv.org/pdf/1609.02907.pdf)). To do so, you will work with PyG's built-in `GCNConv` layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4CcOUEoInjD"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-DCtgcHpGIpd",
    "outputId": "3eb684bb-8a39-4cf6-b3c0-1ae38598925c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "print(torch.__version__)\n",
    "\n",
    "# The PyG built-in GCNConv\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0IK9z0wQIwzQ"
   },
   "source": [
    "## Load and Preprocess the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ibJ0ieoIwQM",
    "outputId": "8d11fa9b-c2ee-406c-8759-f62fbf6c31df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ogbn-arxiv has been updated.\n",
      "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/81 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Stopped downloading due to interruption.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/ogb/utils/url.py:62\u001b[0m, in \u001b[0;36mdownload_url\u001b[0;34m(url, folder, log)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m---> 62\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mread(chunk_size)\n\u001b[1;32m     63\u001b[0m     downloaded_size \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/http/client.py:479\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    478\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 479\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 11\u001b[0m\n\u001b[1;32m      2\u001b[0m dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mogbn-arxiv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load the dataset and transform the adjacency matrix to a sparse tensor.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Previously we loaded the same dataset without transforming the adjacency matrix.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Beside the obvious difference, the adjacency matrix will be stored under different keys:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# The PyG convolution layers support both types of adjacency matrix representations!\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# For the node prediction task we will be using a sparse adjacency matrix\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m dataset \u001b[38;5;241m=\u001b[39m PygNodePropPredDataset(name\u001b[38;5;241m=\u001b[39mdataset_name, transform\u001b[38;5;241m=\u001b[39mT\u001b[38;5;241m.\u001b[39mToSparseTensor())\n\u001b[1;32m     12\u001b[0m data \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Make the adjacency matrix to symmetric\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/ogb/nodeproppred/dataset_pyg.py:68\u001b[0m, in \u001b[0;36mPygNodePropPredDataset.__init__\u001b[0;34m(self, name, root, transform, pre_transform, meta_dict)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_hetero \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis hetero\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28msuper\u001b[39m(PygNodePropPredDataset, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, transform, pre_transform)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_paths[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch_geometric/data/in_memory_dataset.py:81\u001b[0m, in \u001b[0;36mInMemoryDataset.__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter, log, force_reload)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     74\u001b[0m     root: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     force_reload: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     80\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform, pre_transform, pre_filter, log,\n\u001b[1;32m     82\u001b[0m                      force_reload)\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data: Optional[BaseData] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslices: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Tensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch_geometric/data/dataset.py:112\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter, log, force_reload)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforce_reload \u001b[38;5;241m=\u001b[39m force_reload\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_download:\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download()\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_process:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch_geometric/data/dataset.py:229\u001b[0m, in \u001b[0;36mDataset._download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    228\u001b[0m fs\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 229\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/ogb/nodeproppred/dataset_pyg.py:126\u001b[0m, in \u001b[0;36mPygNodePropPredDataset.download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m url \u001b[38;5;241m=\u001b[39m  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decide_download(url):\n\u001b[0;32m--> 126\u001b[0m     path \u001b[38;5;241m=\u001b[39m download_url(url, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_root)\n\u001b[1;32m    127\u001b[0m     extract_zip(path, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_root)\n\u001b[1;32m    128\u001b[0m     os\u001b[38;5;241m.\u001b[39munlink(path)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/ogb/utils/url.py:69\u001b[0m, in \u001b[0;36mdownload_url\u001b[0;34m(url, folder, log)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(path):\n\u001b[1;32m     68\u001b[0m          os\u001b[38;5;241m.\u001b[39mremove(path)\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStopped downloading due to interruption.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m path\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Stopped downloading due to interruption."
     ]
    }
   ],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  dataset_name = 'ogbn-arxiv'\n",
    "\n",
    "  # Load the dataset and transform the adjacency matrix to a sparse tensor.\n",
    "  # Previously we loaded the same dataset without transforming the adjacency matrix.\n",
    "  # Beside the obvious difference, the adjacency matrix will be stored under different keys:\n",
    "  #   - `adj_t` will be the adjacency matrix stored as a sparse Tensor\n",
    "  #   - `edge_index` be the adjacency matrix stored as a regular Tensor\n",
    "  # The PyG convolution layers support both types of adjacency matrix representations!\n",
    "  # For the node prediction task we will be using a sparse adjacency matrix\n",
    "  dataset = PygNodePropPredDataset(name=dataset_name, transform=T.ToSparseTensor())\n",
    "  data = dataset[0]\n",
    "\n",
    "  # Make the adjacency matrix to symmetric\n",
    "  data.adj_t = data.adj_t.to_symmetric()\n",
    "\n",
    "  device = 'cuda' if torch.cuda.is_available()  else 'cpu'\n",
    "\n",
    "  # If you use GPU, the device should be cuda\n",
    "  torch.set_default_device(device)\n",
    "  print('Device: {}'.format(device))\n",
    "\n",
    "  data = data.to(device)\n",
    "  split_idx = dataset.get_idx_split()\n",
    "  train_idx = split_idx['train'].to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgUA815bNJ8w"
   },
   "source": [
    "## GCN Model\n",
    "\n",
    "Now that you have loaded the datasets, you will implement your own GCN model!\n",
    "\n",
    "Please follow the figure below to help in implementing the `forward` function.\n",
    "\n",
    "\n",
    "![test](img.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IgspXTYpNJLA"
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1114686730.py, line 89)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 89\u001b[0;36m\u001b[0m\n\u001b[0;31m    for i in range(len(self.convs) - 1 ):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        hidden_dim,\n",
    "        output_dim,\n",
    "        num_layers,\n",
    "        dropout,\n",
    "        return_embeds=False,\n",
    "    ):\n",
    "\n",
    "        # TODO: Implement the init function that initializes self.convs,\n",
    "        # self.bns, and self.softmax.\n",
    "\n",
    "        super(GCN, self).__init__()\n",
    "        print(\"input_dim\" , input_dim)\n",
    "        print(\"hidden_dim\" , hidden_dim)\n",
    "        print(\"output_dim\" , output_dim)\n",
    "        print(\"num_layers\" , num_layers)\n",
    "        \n",
    "        # A list of GCNConv layers\n",
    "        self.convs = nn.ModuleList(\n",
    "            \n",
    "                [GCNConv(in_channels=input_dim, out_channels=hidden_dim)]+\n",
    "                [\n",
    "                    GCNConv(in_channels=hidden_dim, out_channels=hidden_dim)\n",
    "                    for _ in range(num_layers - 2)\n",
    "                ]+\n",
    "                [GCNConv(in_channels=hidden_dim, out_channels=output_dim)],\n",
    "            \n",
    "        )\n",
    "\n",
    "        # A list of 1D batch normalization layers\n",
    "        self.bns = nn.ModuleList(\n",
    "            [nn.BatchNorm1d(num_features=hidden_dim) for _ in range(num_layers - 1)]\n",
    "           \n",
    "        )\n",
    "\n",
    "        # The log softmax layer\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        ############# Your code here ############\n",
    "        ## Note:\n",
    "        ## 1. Use torch.nn.ModuleList for self.convs and self.bns\n",
    "        ## 2. self.convs has num_layers GCNConv layers\n",
    "        ## 3. self.bns has num_layers - 1 BatchNorm1d layers\n",
    "        ## 4. Use torch.nn.LogSoftmax for self.softmax\n",
    "        ## 5. The GCNConv layer takes as input 'in_channels' and\n",
    "        ## 'out_channels'. For more information please refer to the documentation:\n",
    "        ## https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv\n",
    "        ## 6. The only parameter BatchNorm1d requires is 'num_features'\n",
    "        ## For more information please refer to the documentation:\n",
    "        ## https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html\n",
    "        ## (~10 lines of code)\n",
    "\n",
    "        #########################################\n",
    "\n",
    "        # Probability of an element getting zeroed\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Skip classification layer and return node embeddings\n",
    "        self.return_embeds = return_embeds\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj_t):\n",
    "       \n",
    "        # TODO: Implement a function that takes as input a feature tensor x\n",
    "        # and graph connectivity tensor adj_t, and returns the corresponding output\n",
    "        # tensor as shown in the figure above.\n",
    "\n",
    "        ############# Your code here ############\n",
    "        ## Note:\n",
    "        ## 1. Construct the network as shown in the figure\n",
    "        ## 2. Pass to the conv layer the feature tensor x and the adjacency matrix, adj_t\n",
    "        ## 3. torch.nn.functional.relu and torch.nn.functional.dropout are useful\n",
    "        ## For more information please refer to the documentation:\n",
    "        ## https://pytorch.org/docs/stable/nn.functional.html\n",
    "        ## 4. Don't forget to set F.dropout training to self.training\n",
    "        ## 5. If return_embeds is True, then skip the last softmax layer\n",
    "        ## (~7 lines of code)\n",
    "\n",
    "        #########################################\n",
    "        out = x\n",
    "        for i in range(len(self.convs) - 1 ):\n",
    "            # print(i)\n",
    "            # print(self.convs[i])\n",
    "            # print(self.bns[i])\n",
    "            out = self.convs[i](out, adj_t)\n",
    "            out = self.bns[i](out)\n",
    "            out = F.relu(out)\n",
    "            out = F.dropout(out, p=self.dropout, training=self.training)\n",
    "            \n",
    "            \n",
    "        # out = self.convs(x, adj_t)\n",
    "        # print(out.shape)\n",
    "        \n",
    "          \n",
    "        \n",
    "        # out = self.bns(out)\n",
    "        \n",
    "            out = self.convs[-1](out, adj_t)\n",
    "            if self.return_embeds:\n",
    "                return out\n",
    "\n",
    "        \n",
    "            out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "FF1hnHUhO81e"
   },
   "outputs": [],
   "source": [
    "def train(model, data, train_idx, optimizer, loss_fn):\n",
    "    # TODO: Implement a function that trains the model by \n",
    "    # using the given optimizer and loss_fn.\n",
    "    model.train()\n",
    "    loss = 0\n",
    "\n",
    "    ############# Your code here ############\n",
    "    ## Note:\n",
    "    ## 1. Zero grad the optimizer\n",
    "    ## 2. Feed the data into the model\n",
    "    ## 3. Slice the model outputs and labels by train_idx\n",
    "    ## 4. Feed the sliced outputs and labels to the loss_fn\n",
    "    ## (~4 lines of code)\n",
    "    #########################################\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = model(data.x , data.adj_t)\n",
    "    # print(\"output\" , output.shape)\n",
    "    output_train = output[train_idx]\n",
    "    data_train = data.y[train_idx]\n",
    "    \n",
    "    \n",
    "    # x = output_train.argmax(dim=-1, keepdim=True)\n",
    "    \n",
    "    # print(x.shape , data_train.shape)\n",
    "    loss = loss_fn(output_train, data_train.squeeze())\n",
    "    \n",
    "    \n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "aJdlrJQhPBsK"
   },
   "outputs": [],
   "source": [
    "# Test function here\n",
    "@torch.no_grad()\n",
    "def test(model, data, split_idx, evaluator, save_model_results=False):\n",
    "    # TODO: Implement a function that tests the model by \n",
    "    # using the given split_idx and ogb evaluator.\n",
    "    model.eval()\n",
    "\n",
    "    # The output of model on all data\n",
    "    out = model(data.x, data.adj_t)\n",
    "\n",
    "    ############# Your code here ############\n",
    "    ## (~1 line of code)\n",
    "    ## Note:\n",
    "    ## 1. No index slicing here\n",
    "    \n",
    "    #########################################\n",
    "\n",
    "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "    train_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['train']],\n",
    "        'y_pred': y_pred[split_idx['train']],\n",
    "    })['acc']\n",
    "    valid_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['valid']],\n",
    "        'y_pred': y_pred[split_idx['valid']],\n",
    "    })['acc']\n",
    "    test_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['test']],\n",
    "        'y_pred': y_pred[split_idx['test']],\n",
    "    })['acc']\n",
    "\n",
    "    if save_model_results:\n",
    "      print (\"Saving Model Predictions\")\n",
    "\n",
    "      data = {}\n",
    "      data['y_pred'] = y_pred.view(-1).cpu().detach().numpy()\n",
    "\n",
    "      df = pd.DataFrame(data=data)\n",
    "      # Save locally as csv\n",
    "      df.to_csv('ogbn-arxiv_node.csv', sep=',', index=False)\n",
    "\n",
    "\n",
    "    return train_acc, valid_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "o7F46xkuLiOL"
   },
   "outputs": [],
   "source": [
    "# Please do not change the args\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  args = {\n",
    "      'device': device,\n",
    "      'num_layers': 3,\n",
    "      'hidden_dim': 256,\n",
    "      'dropout': 0.5,\n",
    "      'lr': 0.01,\n",
    "      'epochs': 100,\n",
    "  }\n",
    "  args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "dT8RyM2cPGxM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim 128\n",
      "hidden_dim 256\n",
      "output_dim 40\n",
      "num_layers 3\n"
     ]
    }
   ],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  model = GCN(data.num_features, args['hidden_dim'],\n",
    "              dataset.num_classes, args['num_layers'],\n",
    "              args['dropout']).to(device)\n",
    "  # Disable compile as this does not seem to work yet in PyTorch 2.0.1/PyG 2.3.1\n",
    "  # try:\n",
    "  #   model = torch_geometric.compile(model)\n",
    "  #   print(\"GCN Model compiled\")\n",
    "  # except Exception as err:\n",
    "  #   print(f\"Model compile not supported: {err}\")\n",
    "\n",
    "  evaluator = Evaluator(name='ogbn-arxiv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (convs): ModuleList(\n",
       "    (0): GCNConv(128, 256)\n",
       "    (1): GCNConv(256, 256)\n",
       "    (2): GCNConv(256, 40)\n",
       "  )\n",
       "  (bns): ModuleList(\n",
       "    (0-1): 2 x BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (softmax): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qd5O5cnPPdVF",
    "outputId": "aca5420f-b821-40c2-d1fd-3b009eaf75d2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/utils/_device.py:78: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:55.)\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 3.8988, Train: 28.17%, Valid: 30.26% Test: 27.06%\n",
      "Epoch: 02, Loss: 2.3435, Train: 27.39%, Valid: 28.39% Test: 32.24%\n",
      "Epoch: 03, Loss: 1.9232, Train: 27.09%, Valid: 25.97% Test: 28.51%\n",
      "Epoch: 04, Loss: 1.7593, Train: 31.84%, Valid: 30.54% Test: 35.43%\n",
      "Epoch: 05, Loss: 1.6430, Train: 39.11%, Valid: 34.83% Test: 39.55%\n",
      "Epoch: 06, Loss: 1.5750, Train: 40.95%, Valid: 32.01% Test: 35.52%\n",
      "Epoch: 07, Loss: 1.5066, Train: 40.88%, Valid: 33.01% Test: 36.55%\n",
      "Epoch: 08, Loss: 1.4556, Train: 40.33%, Valid: 32.78% Test: 35.77%\n",
      "Epoch: 09, Loss: 1.4134, Train: 41.62%, Valid: 33.41% Test: 36.65%\n",
      "Epoch: 10, Loss: 1.3762, Train: 43.13%, Valid: 34.98% Test: 38.75%\n",
      "Epoch: 11, Loss: 1.3488, Train: 44.79%, Valid: 38.92% Test: 43.30%\n",
      "Epoch: 12, Loss: 1.3209, Train: 46.22%, Valid: 41.64% Test: 45.97%\n",
      "Epoch: 13, Loss: 1.2935, Train: 46.95%, Valid: 42.00% Test: 46.39%\n",
      "Epoch: 14, Loss: 1.2717, Train: 48.04%, Valid: 42.47% Test: 46.93%\n",
      "Epoch: 15, Loss: 1.2545, Train: 49.33%, Valid: 43.69% Test: 48.08%\n",
      "Epoch: 16, Loss: 1.2372, Train: 51.66%, Valid: 47.58% Test: 51.82%\n",
      "Epoch: 17, Loss: 1.2227, Train: 54.67%, Valid: 52.62% Test: 56.66%\n",
      "Epoch: 18, Loss: 1.2125, Train: 56.68%, Valid: 56.04% Test: 59.82%\n",
      "Epoch: 19, Loss: 1.2017, Train: 57.95%, Valid: 57.79% Test: 61.33%\n",
      "Epoch: 20, Loss: 1.1855, Train: 58.15%, Valid: 57.49% Test: 60.98%\n",
      "Epoch: 21, Loss: 1.1724, Train: 58.38%, Valid: 56.96% Test: 60.56%\n",
      "Epoch: 22, Loss: 1.1669, Train: 59.04%, Valid: 57.54% Test: 60.90%\n",
      "Epoch: 23, Loss: 1.1559, Train: 59.72%, Valid: 58.33% Test: 60.82%\n",
      "Epoch: 24, Loss: 1.1460, Train: 60.66%, Valid: 59.15% Test: 60.89%\n",
      "Epoch: 25, Loss: 1.1351, Train: 61.44%, Valid: 59.96% Test: 61.38%\n",
      "Epoch: 26, Loss: 1.1317, Train: 62.44%, Valid: 61.38% Test: 62.80%\n",
      "Epoch: 27, Loss: 1.1201, Train: 63.58%, Valid: 63.16% Test: 64.57%\n",
      "Epoch: 28, Loss: 1.1137, Train: 64.64%, Valid: 64.55% Test: 65.75%\n",
      "Epoch: 29, Loss: 1.1097, Train: 65.38%, Valid: 65.32% Test: 66.41%\n",
      "Epoch: 30, Loss: 1.0994, Train: 65.96%, Valid: 66.04% Test: 66.95%\n",
      "Epoch: 31, Loss: 1.0931, Train: 66.30%, Valid: 66.33% Test: 67.13%\n",
      "Epoch: 32, Loss: 1.0869, Train: 66.64%, Valid: 66.34% Test: 66.82%\n",
      "Epoch: 33, Loss: 1.0819, Train: 66.80%, Valid: 66.06% Test: 66.29%\n",
      "Epoch: 34, Loss: 1.0767, Train: 67.10%, Valid: 66.01% Test: 66.05%\n",
      "Epoch: 35, Loss: 1.0742, Train: 67.64%, Valid: 66.83% Test: 66.61%\n",
      "Epoch: 36, Loss: 1.0657, Train: 67.81%, Valid: 67.45% Test: 67.35%\n",
      "Epoch: 37, Loss: 1.0622, Train: 67.55%, Valid: 67.37% Test: 67.72%\n",
      "Epoch: 38, Loss: 1.0554, Train: 67.09%, Valid: 66.59% Test: 67.23%\n",
      "Epoch: 39, Loss: 1.0509, Train: 67.42%, Valid: 66.94% Test: 67.45%\n",
      "Epoch: 40, Loss: 1.0498, Train: 68.41%, Valid: 67.94% Test: 68.07%\n",
      "Epoch: 41, Loss: 1.0464, Train: 68.99%, Valid: 68.69% Test: 68.51%\n",
      "Epoch: 42, Loss: 1.0422, Train: 69.52%, Valid: 69.22% Test: 68.85%\n",
      "Epoch: 43, Loss: 1.0353, Train: 69.71%, Valid: 69.43% Test: 68.94%\n",
      "Epoch: 44, Loss: 1.0357, Train: 69.93%, Valid: 69.60% Test: 69.17%\n",
      "Epoch: 45, Loss: 1.0289, Train: 70.20%, Valid: 69.81% Test: 69.53%\n",
      "Epoch: 46, Loss: 1.0261, Train: 70.51%, Valid: 69.98% Test: 69.62%\n",
      "Epoch: 47, Loss: 1.0230, Train: 70.70%, Valid: 69.91% Test: 69.20%\n",
      "Epoch: 48, Loss: 1.0212, Train: 70.82%, Valid: 70.02% Test: 69.31%\n",
      "Epoch: 49, Loss: 1.0144, Train: 70.94%, Valid: 70.21% Test: 69.50%\n",
      "Epoch: 50, Loss: 1.0153, Train: 71.20%, Valid: 70.37% Test: 69.64%\n",
      "Epoch: 51, Loss: 1.0085, Train: 71.33%, Valid: 70.57% Test: 69.67%\n",
      "Epoch: 52, Loss: 1.0082, Train: 71.49%, Valid: 70.81% Test: 69.82%\n",
      "Epoch: 53, Loss: 1.0028, Train: 71.60%, Valid: 70.86% Test: 70.01%\n",
      "Epoch: 54, Loss: 1.0023, Train: 71.62%, Valid: 70.91% Test: 70.38%\n",
      "Epoch: 55, Loss: 0.9965, Train: 71.81%, Valid: 71.07% Test: 70.34%\n",
      "Epoch: 56, Loss: 0.9943, Train: 71.88%, Valid: 71.12% Test: 70.24%\n",
      "Epoch: 57, Loss: 0.9934, Train: 71.90%, Valid: 71.05% Test: 70.32%\n",
      "Epoch: 58, Loss: 0.9892, Train: 71.88%, Valid: 71.09% Test: 70.73%\n",
      "Epoch: 59, Loss: 0.9875, Train: 71.71%, Valid: 70.76% Test: 70.63%\n",
      "Epoch: 60, Loss: 0.9866, Train: 71.81%, Valid: 70.81% Test: 70.45%\n",
      "Epoch: 61, Loss: 0.9812, Train: 71.90%, Valid: 70.72% Test: 70.09%\n",
      "Epoch: 62, Loss: 0.9792, Train: 72.08%, Valid: 70.62% Test: 69.56%\n",
      "Epoch: 63, Loss: 0.9777, Train: 72.13%, Valid: 70.44% Test: 69.07%\n",
      "Epoch: 64, Loss: 0.9763, Train: 72.24%, Valid: 70.80% Test: 69.89%\n",
      "Epoch: 65, Loss: 0.9743, Train: 72.14%, Valid: 70.99% Test: 70.60%\n",
      "Epoch: 66, Loss: 0.9710, Train: 71.95%, Valid: 70.80% Test: 70.50%\n",
      "Epoch: 67, Loss: 0.9696, Train: 72.17%, Valid: 71.07% Test: 70.37%\n",
      "Epoch: 68, Loss: 0.9662, Train: 72.45%, Valid: 71.27% Test: 70.21%\n",
      "Epoch: 69, Loss: 0.9650, Train: 72.61%, Valid: 71.42% Test: 70.58%\n",
      "Epoch: 70, Loss: 0.9620, Train: 72.54%, Valid: 71.49% Test: 70.87%\n",
      "Epoch: 71, Loss: 0.9615, Train: 72.63%, Valid: 71.44% Test: 70.57%\n",
      "Epoch: 72, Loss: 0.9591, Train: 72.62%, Valid: 71.15% Test: 70.00%\n",
      "Epoch: 73, Loss: 0.9584, Train: 72.52%, Valid: 70.84% Test: 69.48%\n",
      "Epoch: 74, Loss: 0.9534, Train: 72.53%, Valid: 71.08% Test: 69.66%\n",
      "Epoch: 75, Loss: 0.9511, Train: 72.57%, Valid: 71.17% Test: 70.23%\n",
      "Epoch: 76, Loss: 0.9507, Train: 72.63%, Valid: 71.36% Test: 70.70%\n",
      "Epoch: 77, Loss: 0.9524, Train: 72.80%, Valid: 71.47% Test: 70.64%\n",
      "Epoch: 78, Loss: 0.9478, Train: 72.95%, Valid: 71.41% Test: 69.89%\n",
      "Epoch: 79, Loss: 0.9453, Train: 72.96%, Valid: 71.60% Test: 70.32%\n",
      "Epoch: 80, Loss: 0.9448, Train: 72.81%, Valid: 71.60% Test: 71.08%\n",
      "Epoch: 81, Loss: 0.9424, Train: 72.91%, Valid: 71.34% Test: 70.84%\n",
      "Epoch: 82, Loss: 0.9417, Train: 73.21%, Valid: 71.37% Test: 70.28%\n",
      "Epoch: 83, Loss: 0.9411, Train: 73.11%, Valid: 70.71% Test: 68.85%\n",
      "Epoch: 84, Loss: 0.9402, Train: 73.20%, Valid: 71.09% Test: 69.65%\n",
      "Epoch: 85, Loss: 0.9344, Train: 73.18%, Valid: 71.56% Test: 70.81%\n",
      "Epoch: 86, Loss: 0.9367, Train: 73.07%, Valid: 71.27% Test: 70.80%\n",
      "Epoch: 87, Loss: 0.9326, Train: 73.29%, Valid: 71.27% Test: 70.01%\n",
      "Epoch: 88, Loss: 0.9299, Train: 73.19%, Valid: 71.15% Test: 69.42%\n",
      "Epoch: 89, Loss: 0.9345, Train: 73.24%, Valid: 71.50% Test: 70.37%\n",
      "Epoch: 90, Loss: 0.9290, Train: 73.28%, Valid: 71.82% Test: 71.26%\n",
      "Epoch: 91, Loss: 0.9284, Train: 73.43%, Valid: 71.91% Test: 71.23%\n",
      "Epoch: 92, Loss: 0.9255, Train: 73.61%, Valid: 71.78% Test: 70.45%\n",
      "Epoch: 93, Loss: 0.9259, Train: 73.37%, Valid: 71.45% Test: 70.06%\n",
      "Epoch: 94, Loss: 0.9254, Train: 73.12%, Valid: 71.14% Test: 69.95%\n",
      "Epoch: 95, Loss: 0.9209, Train: 73.33%, Valid: 71.46% Test: 69.92%\n",
      "Epoch: 96, Loss: 0.9225, Train: 73.50%, Valid: 71.43% Test: 69.51%\n",
      "Epoch: 97, Loss: 0.9202, Train: 73.55%, Valid: 71.72% Test: 70.39%\n",
      "Epoch: 98, Loss: 0.9179, Train: 73.56%, Valid: 71.61% Test: 71.13%\n",
      "Epoch: 99, Loss: 0.9142, Train: 73.56%, Valid: 71.59% Test: 70.74%\n",
      "Epoch: 100, Loss: 0.9160, Train: 73.45%, Valid: 70.11% Test: 67.68%\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  # reset the parameters to initial random value\n",
    "  model.reset_parameters()\n",
    "\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
    "  loss_fn = F.nll_loss\n",
    "\n",
    "  best_model = None\n",
    "  best_valid_acc = 0\n",
    "\n",
    "  for epoch in range(1, 1 + args[\"epochs\"]):\n",
    "    loss = train(model, data, train_idx, optimizer, loss_fn)\n",
    "    result = test(model, data, split_idx, evaluator)\n",
    "    train_acc, valid_acc, test_acc = result\n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        best_model = copy.deepcopy(model)\n",
    "    print(f'Epoch: {epoch:02d}, '\n",
    "          f'Loss: {loss:.4f}, '\n",
    "          f'Train: {100 * train_acc:.2f}%, '\n",
    "          f'Valid: {100 * valid_acc:.2f}% '\n",
    "          f'Test: {100 * test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQtt-EKA8P4r"
   },
   "source": [
    "## Question 5: What are your `best_model` validation and test accuracies?(20 points)\n",
    "\n",
    "Run the cell below to see the results of your best model and save your model's predictions to a file named *ogbn-arxiv_node.csv*. \n",
    "\n",
    "You can view this file by clicking on the *Folder* icon on the left side pannel. As in Colab 1, when you sumbit your assignment, you will have to download this file and attatch it to your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EqcextqOL2FX",
    "outputId": "168cdb4a-539a-4b40-d052-e85a9616ae0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model Predictions\n",
      "Best model: Train: 73.43%, Valid: 71.91% Test: 71.23%\n"
     ]
    }
   ],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  best_result = test(best_model, data, split_idx, evaluator, save_model_results=True)\n",
    "  train_acc, valid_acc, test_acc = best_result\n",
    "  print(f'Best model: '\n",
    "        f'Train: {100 * train_acc:.2f}%, '\n",
    "        f'Valid: {100 * valid_acc:.2f}% '\n",
    "        f'Test: {100 * test_acc:.2f}%')\n",
    "# Saving Model Predictions\n",
    "# Best model: Train: 65.66%, Valid: 66.07% Test: 64.54%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8pOD6y80TyI"
   },
   "source": [
    "# 4) GNN: Graph Property Prediction\n",
    "\n",
    "In this section you will create a graph neural network for graph property prediction (graph classification).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRg5VOEdQTa4"
   },
   "source": [
    "## Load and preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LXb-O5QUIgTH",
    "outputId": "35253c63-fbfd-4988-8442-1e089c8ac464"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Task type: binary classification\n"
     ]
    }
   ],
   "source": [
    "from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n",
    "from torch_geometric.loader import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  # Load the dataset without transforming the adjacency matrix.\n",
    "  # Without any transformation of the adjacency matrix, as specified in the beginning of this colab,\n",
    "  # the adjacency matrix will be stored under the `edge_index` key as a regular Tensor\n",
    "  dataset = PygGraphPropPredDataset(name='ogbg-molhiv', transform=None)\n",
    "\n",
    "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "  print('Device: {}'.format(device))\n",
    "\n",
    "  split_idx = dataset.get_idx_split()\n",
    "\n",
    "  # Check task type\n",
    "  print('Task type: {}'.format(dataset.task_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "7cHHbgW1c5hi"
   },
   "outputs": [],
   "source": [
    "# Load the dataset splits into corresponding dataloaders\n",
    "# We will train the graph classification task on a batch of 32 graphs\n",
    "# Shuffle the order of graphs for training set\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  train_loader = DataLoader(dataset[split_idx[\"train\"]], batch_size=32, shuffle=True, num_workers=0)\n",
    "  valid_loader = DataLoader(dataset[split_idx[\"valid\"]], batch_size=32, shuffle=False, num_workers=0)\n",
    "  test_loader = DataLoader(dataset[split_idx[\"test\"]], batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbcBPojdPfc-"
   },
   "source": [
    "## Initialize Model Training Parameters\n",
    "During debugging and testing we recommend setting `epochs` to a lower value such as 1 or 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "AYrSnOj0Y4DK"
   },
   "outputs": [],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  # Please do not change the args\n",
    "  args = {\n",
    "      'device': device,\n",
    "      'num_layers': 5,\n",
    "      'hidden_dim': 256,\n",
    "      'dropout': 0.5,\n",
    "      'lr': 0.001,\n",
    "      'epochs': 15,\n",
    "  }\n",
    "  args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WLhguSTeazy"
   },
   "source": [
    "## Graph Prediction Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u05Z14TRYPGn"
   },
   "source": [
    "### Graph Mini-Batching\n",
    "Before diving into the actual model, we introduce the concept of mini-batching with graphs. In order to parallelize the processing of a mini-batch of graphs, PyG combines the graphs into a single disconnected graph data object (*torch_geometric.data.Batch*). *torch_geometric.data.Batch* inherits from *torch_geometric.data.Data* (introduced earlier) and contains an additional attribute called `batch`. \n",
    "\n",
    "The `batch` attribute is a vector mapping each node to the index of its corresponding graph within the mini-batch:\n",
    "\n",
    "    batch = [0, ..., 0, 1, ..., n - 2, n - 1, ..., n - 1]\n",
    "\n",
    "This attribute is crucial for associating which graph each node belongs to and can be used to e.g. average the node embeddings for each graph individually to compute graph level embeddings. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pcic9NNU3nGK"
   },
   "source": [
    "### Implementation\n",
    "Now, you have all of the tools to implement a GCN Graph Prediction model!  \n",
    "\n",
    "To do so, you will reuse the your existing GCN model to generate `node_embeddings` for a graph and then use `Global Pooling` over these node embeddings to create a graph level embeddings that can be used to predict graph properties. Remeber that the `batch` attribute will be essential for performining Global Pooling over our mini-batch of graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrrJ2TgNUIhf"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "3_Kq3zyjeZ22"
   },
   "outputs": [],
   "source": [
    "from ogb.graphproppred.mol_encoder import AtomEncoder\n",
    "from torch_geometric.nn import global_add_pool, global_mean_pool\n",
    "\n",
    "### GCN to predict graph property\n",
    "class GCN_Graph(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, num_layers, dropout):\n",
    "        super(GCN_Graph, self).__init__()\n",
    "\n",
    "        # Load encoders for Atoms in molecule graphs\n",
    "        self.node_encoder = AtomEncoder(hidden_dim)\n",
    "\n",
    "        # Node embedding model\n",
    "        # Note that the input_dim and output_dim are set to hidden_dim\n",
    "        self.gnn_node = GCN(hidden_dim, hidden_dim,\n",
    "            hidden_dim, num_layers, dropout, return_embeds=True)\n",
    "\n",
    "        self.pool = global_mean_pool\n",
    "\n",
    "        ############# Your code here ############\n",
    "        ## Note:\n",
    "        ## 1. Initialize self.pool as a reference to a global mean pooling layer\n",
    "        ## For more information please refer to the documentation:\n",
    "        ## https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#global-pooling-layers\n",
    "        \n",
    "        #########################################\n",
    "\n",
    "        # Output layer\n",
    "        self.linear = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "      self.gnn_node.reset_parameters()\n",
    "      self.linear.reset_parameters()\n",
    "\n",
    "    def forward(self, batched_data):\n",
    "        # TODO: Implement a function that takes as input a \n",
    "        # mini-batch of graphs (torch_geometric.data.Batch) and \n",
    "        # returns the predicted graph property for each graph. \n",
    "        #\n",
    "        # NOTE: Since we are predicting graph level properties,\n",
    "        # your output will be a tensor with dimension equaling\n",
    "        # the number of graphs in the mini-batch\n",
    "\n",
    "    \n",
    "        # Extract important attributes of our mini-batch\n",
    "        x, edge_index, batch = batched_data.x, batched_data.edge_index, batched_data.batch\n",
    "        embed = self.node_encoder(x)\n",
    "\n",
    "        out = self.gnn_node(embed , edge_index)\n",
    "        out = self.pool(out , batch)\n",
    "\n",
    "        ############# Your code here ############\n",
    "        ## Note:\n",
    "        ## 1. Construct node embeddings using your existing GCN model and \n",
    "        ## pass it the feature tensor, embed, and the adjacency matrix, edge_index\n",
    "        ## 2. Use the global pooling layer to aggregate features for each individual graph\n",
    "        ## For more information please refer to the documentation:\n",
    "        ## https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#global-pooling-layers\n",
    "        ## 3. Use a linear layer to predict each graph's property\n",
    "        ## (~3 lines of code)\n",
    "        out = self.linear(out)\n",
    "        #########################################\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "FJjnGuMSbjX0"
   },
   "outputs": [],
   "source": [
    "def train(model, device, data_loader, optimizer, loss_fn):\n",
    "    # TODO: Implement a function that trains your model by \n",
    "    # using the given optimizer and loss_fn.\n",
    "    model.train()\n",
    "    loss = 0\n",
    "\n",
    "    for step, batch in enumerate(tqdm(data_loader, desc=\"Iteration\")):\n",
    "      batch = batch.to(device)\n",
    "\n",
    "      if batch.x.shape[0] == 1 or batch.batch[-1] == 0:\n",
    "          pass\n",
    "      else:\n",
    "        ## ignore nan targets (unlabeled) when computing training loss.\n",
    "        is_labeled = batch.y == batch.y\n",
    "\n",
    "        ############# Your code here ############\n",
    "        ## Note:\n",
    "        ## 1. Zero grad the optimizer\n",
    "        ## 2. Feed the data into the model\n",
    "        ## 3. Use `is_labeled` mask to filter outputs and labels\n",
    "        ## 4. You may need to change the type of label to torch.float32\n",
    "        ## 5. Feed the output and label to the loss_fn\n",
    "        ## (~3 lines of code)\n",
    "        #########################################\n",
    "        optimizer.zero_grad()\n",
    "        # print(batch)\n",
    "        output = model(batch)\n",
    "        \n",
    "        output_filtered = output[is_labeled]\n",
    "        label_filtered = batch.y[is_labeled]\n",
    "        \n",
    "        loss = loss_fn(output_filtered , label_filtered.type(torch.float32))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "ztPHXq_Gzn7U"
   },
   "outputs": [],
   "source": [
    "# The evaluation function\n",
    "def eval(model, device, loader, evaluator, save_model_results=False, save_file=None):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for step, batch in enumerate(tqdm(loader, desc=\"Iteration\")):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        if batch.x.shape[0] == 1:\n",
    "            pass\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                pred = model(batch)\n",
    "\n",
    "            y_true.append(batch.y.view(pred.shape).detach().cpu())\n",
    "            y_pred.append(pred.detach().cpu())\n",
    "\n",
    "    y_true = torch.cat(y_true, dim = 0).numpy()\n",
    "    y_pred = torch.cat(y_pred, dim = 0).numpy()\n",
    "\n",
    "    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n",
    "\n",
    "    if save_model_results:\n",
    "        print (\"Saving Model Predictions\")\n",
    "        \n",
    "        # Create a pandas dataframe with a two columns\n",
    "        # y_pred | y_true\n",
    "        data = {}\n",
    "        data['y_pred'] = y_pred.reshape(-1)\n",
    "        data['y_true'] = y_true.reshape(-1)\n",
    "\n",
    "        df = pd.DataFrame(data=data)\n",
    "        # Save to csv\n",
    "        df.to_csv('ogbg-molhiv_graph_' + save_file + '.csv', sep=',', index=False)\n",
    "\n",
    "    return evaluator.eval(input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "MR1wQ4hMZeMw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim 256\n",
      "hidden_dim 256\n",
      "output_dim 256\n",
      "num_layers 5\n"
     ]
    }
   ],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  model = GCN_Graph(args['hidden_dim'],\n",
    "              dataset.num_tasks, args['num_layers'],\n",
    "              args['dropout']).to(device)\n",
    "  # Disable compile as this does not seem to work yet in PyTorch 2.0.1/PyG 2.3.1\n",
    "  # try:\n",
    "  #   model = torch_geometric.compile(model)\n",
    "  #   print(\"Graph Prediction Model compiled\")\n",
    "  # except Exception as err:\n",
    "  #   print(f\"Model compile not supported: {err}\")\n",
    "\n",
    "  evaluator = Evaluator(name='ogbg-molhiv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qJGTNZiuZy0A",
    "outputId": "dd186307-5f9b-4884-f897-7b1a5d9bbdc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1029/1029 [00:41<00:00, 24.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1029/1029 [00:14<00:00, 72.31it/s]\n",
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:02<00:00, 47.36it/s]\n",
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:02<00:00, 51.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 0.0235, Train: 70.71%, Valid: 70.61% Test: 70.55%\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1029/1029 [00:38<00:00, 26.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1029/1029 [00:16<00:00, 61.38it/s]\n",
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:02<00:00, 60.44it/s]\n",
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:01<00:00, 68.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02, Loss: 0.9646, Train: 74.07%, Valid: 72.98% Test: 73.32%\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1029/1029 [00:37<00:00, 27.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1029/1029 [00:14<00:00, 72.27it/s]\n",
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:02<00:00, 59.81it/s]\n",
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:02<00:00, 63.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03, Loss: 0.7650, Train: 76.47%, Valid: 73.18% Test: 70.65%\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1029/1029 [00:36<00:00, 27.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1029/1029 [00:13<00:00, 75.18it/s]\n",
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:01<00:00, 71.94it/s]\n",
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:01<00:00, 75.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04, Loss: 0.0315, Train: 77.13%, Valid: 77.13% Test: 72.88%\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1029/1029 [00:35<00:00, 28.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1029/1029 [00:13<00:00, 74.61it/s]\n",
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:01<00:00, 71.72it/s]\n",
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:01<00:00, 74.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05, Loss: 0.8540, Train: 78.29%, Valid: 75.85% Test: 70.87%\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1029/1029 [00:35<00:00, 28.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1029/1029 [00:13<00:00, 75.83it/s]\n",
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:01<00:00, 68.06it/s]\n",
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:01<00:00, 76.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06, Loss: 0.0352, Train: 77.91%, Valid: 72.85% Test: 71.96%\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1029/1029 [00:37<00:00, 27.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1029/1029 [00:14<00:00, 73.35it/s]\n",
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:01<00:00, 69.65it/s]\n",
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:01<00:00, 73.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07, Loss: 0.0505, Train: 77.01%, Valid: 74.99% Test: 68.22%\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1029/1029 [00:36<00:00, 28.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1029/1029 [00:14<00:00, 73.22it/s]\n",
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:01<00:00, 68.96it/s]\n",
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:01<00:00, 75.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08, Loss: 0.0219, Train: 78.53%, Valid: 76.95% Test: 71.21%\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1029/1029 [00:37<00:00, 27.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1029/1029 [00:14<00:00, 72.32it/s]\n",
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:01<00:00, 69.49it/s]\n",
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:01<00:00, 73.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09, Loss: 0.0252, Train: 79.77%, Valid: 73.71% Test: 72.18%\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1029/1029 [00:37<00:00, 27.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1029/1029 [00:14<00:00, 72.48it/s]\n",
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:01<00:00, 68.96it/s]\n",
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:01<00:00, 72.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Loss: 0.0312, Train: 79.85%, Valid: 76.48% Test: 71.69%\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1029/1029 [00:36<00:00, 27.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1029/1029 [00:14<00:00, 71.79it/s]\n",
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:01<00:00, 68.45it/s]\n",
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:01<00:00, 73.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Loss: 0.0219, Train: 80.38%, Valid: 74.26% Test: 72.78%\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1029/1029 [00:37<00:00, 27.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1029/1029 [00:14<00:00, 71.60it/s]\n",
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:01<00:00, 66.73it/s]\n",
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:01<00:00, 73.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Loss: 0.0417, Train: 79.72%, Valid: 76.12% Test: 70.44%\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1029/1029 [00:38<00:00, 26.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1029/1029 [00:14<00:00, 69.81it/s]\n",
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:01<00:00, 66.83it/s]\n",
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:01<00:00, 73.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Loss: 0.0217, Train: 79.72%, Valid: 73.81% Test: 73.49%\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1029/1029 [00:36<00:00, 28.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1029/1029 [00:14<00:00, 70.59it/s]\n",
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:01<00:00, 67.02it/s]\n",
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:01<00:00, 71.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Loss: 0.0359, Train: 80.69%, Valid: 77.55% Test: 73.23%\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1029/1029 [00:36<00:00, 27.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1029/1029 [00:14<00:00, 72.43it/s]\n",
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:02<00:00, 60.42it/s]\n",
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:01<00:00, 70.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Loss: 0.0457, Train: 81.96%, Valid: 75.89% Test: 73.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  model.reset_parameters()\n",
    "\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
    "  loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "  best_model = None\n",
    "  best_valid_acc = 0\n",
    "\n",
    "  for epoch in range(1, 1 + args[\"epochs\"]):\n",
    "    print('Training...')\n",
    "    loss = train(model, device, train_loader, optimizer, loss_fn)\n",
    "\n",
    "    print('Evaluating...')\n",
    "    train_result = eval(model, device, train_loader, evaluator)\n",
    "    val_result = eval(model, device, valid_loader, evaluator)\n",
    "    test_result = eval(model, device, test_loader, evaluator)\n",
    "\n",
    "    train_acc, valid_acc, test_acc = train_result[dataset.eval_metric], val_result[dataset.eval_metric], test_result[dataset.eval_metric]\n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        best_model = copy.deepcopy(model)\n",
    "    print(f'Epoch: {epoch:02d}, '\n",
    "          f'Loss: {loss:.4f}, '\n",
    "          f'Train: {100 * train_acc:.2f}%, '\n",
    "          f'Valid: {100 * valid_acc:.2f}% '\n",
    "          f'Test: {100 * test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6I17-Qso_n88"
   },
   "source": [
    "## Question 6: What are your `best_model` validation and test ROC-AUC scores? (20 points)\n",
    "\n",
    "Run the cell below to see the results of your best model and save your model's predictions over the validation and test datasets. The resulting files are named *ogbg-molhiv_graph_valid.csv* and *ogbg-molhiv_graph_test.csv*. \n",
    "\n",
    "Again, you can view these files by clicking on the *Folder* icon on the left side pannel. As in Colab 1, when you sumbit your assignment, you will have to download these files and attatch them to your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oq5QaG21dOOO",
    "outputId": "b5941e39-3162-4764-bc25-d9dd367cc098"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1029/1029 [00:13<00:00, 74.28it/s]\n",
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:01<00:00, 68.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 129/129 [00:01<00:00, 73.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model Predictions\n",
      "Best model: Train: 80.69%, Valid: 77.55% Test: 73.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  train_acc = eval(best_model, device, train_loader, evaluator)[dataset.eval_metric]\n",
    "  valid_acc = eval(best_model, device, valid_loader, evaluator, save_model_results=True, save_file=\"valid\")[dataset.eval_metric]\n",
    "  test_acc  = eval(best_model, device, test_loader, evaluator, save_model_results=True, save_file=\"test\")[dataset.eval_metric]\n",
    "\n",
    "  print(f'Best model: '\n",
    "      f'Train: {100 * train_acc:.2f}%, '\n",
    "      f'Valid: {100 * valid_acc:.2f}% '\n",
    "      f'Test: {100 * test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBi_t8n0iZ4P"
   },
   "source": [
    "## Question 7 (Optional): Experiment with the two other global pooling layers in Pytorch Geometric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# global_add_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ogb.graphproppred.mol_encoder import AtomEncoder\n",
    "# from torch_geometric.nn import global_add_pool, global_mean_pool\n",
    "\n",
    "\n",
    "# class GCN_Graph_Add_Pool(torch.nn.Module):\n",
    "#     def __init__(self, hidden_dim, output_dim, num_layers, dropout):\n",
    "#         super(GCN_Graph_Add_Pool, self).__init__()\n",
    "\n",
    "        \n",
    "#         self.node_encoder = AtomEncoder(hidden_dim)\n",
    "#         self.gnn_node = GCN(hidden_dim, hidden_dim,\n",
    "#             hidden_dim, num_layers, dropout, return_embeds=True)\n",
    "\n",
    "#         self.pool = global_add_pool\n",
    "#         self.linear = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "\n",
    "#     def reset_parameters(self):\n",
    "#       self.gnn_node.reset_parameters()\n",
    "#       self.linear.reset_parameters()\n",
    "\n",
    "#     def forward(self, batched_data):\n",
    "       \n",
    "#         x, edge_index, batch = batched_data.x, batched_data.edge_index, batched_data.batch\n",
    "#         embed = self.node_encoder(x)\n",
    "\n",
    "#         out = self.gnn_node(embed , edge_index)\n",
    "#         out = self.pool(out , batch)\n",
    "\n",
    "#         out = self.linear(out)\n",
    "      \n",
    "\n",
    "#         return out\n",
    "    \n",
    "# model = GCN_Graph_Add_Pool(args['hidden_dim'],\n",
    "#               dataset.num_tasks, args['num_layers'],\n",
    "#               args['dropout']).to(device)\n",
    "  \n",
    "\n",
    "# evaluator = Evaluator(name='ogbg-molhiv')\n",
    "# model.reset_parameters()\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
    "# loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# best_model = None\n",
    "# best_valid_acc = 0\n",
    "\n",
    "# for epoch in range(1, 1 + args[\"epochs\"]):\n",
    "#   print('Training...')\n",
    "#   loss = train(model, device, train_loader, optimizer, loss_fn)\n",
    "\n",
    "#   print('Evaluating...')\n",
    "#   train_result = eval(model, device, train_loader, evaluator)\n",
    "#   val_result = eval(model, device, valid_loader, evaluator)\n",
    "#   test_result = eval(model, device, test_loader, evaluator)\n",
    "\n",
    "#   train_acc, valid_acc, test_acc = train_result[dataset.eval_metric], val_result[dataset.eval_metric], test_result[dataset.eval_metric]\n",
    "#   if valid_acc > best_valid_acc:\n",
    "#       best_valid_acc = valid_acc\n",
    "#       best_model = copy.deepcopy(model)\n",
    "#   print(f'Epoch: {epoch:02d}, '\n",
    "#         f'Loss: {loss:.4f}, '\n",
    "#         f'Train: {100 * train_acc:.2f}%, '\n",
    "#         f'Valid: {100 * valid_acc:.2f}% '\n",
    "#         f'Test: {100 * test_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_acc = eval(best_model, device, train_loader, evaluator)[dataset.eval_metric]\n",
    "# valid_acc = eval(best_model, device, valid_loader, evaluator, save_model_results=True, save_file=\"valid_global_add_pool\")[dataset.eval_metric]\n",
    "# test_acc  = eval(best_model, device, test_loader, evaluator, save_model_results=True, save_file=\"test_global_add_pool\")[dataset.eval_metric]\n",
    "\n",
    "# print(f'Best model: '\n",
    "#     f'Train: {100 * train_acc:.2f}%, '\n",
    "#     f'Valid: {100 * valid_acc:.2f}% '\n",
    "#     f'Test: {100 * test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# global_max_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ogb.graphproppred.mol_encoder import AtomEncoder\n",
    "# from torch_geometric.nn import global_add_pool, global_mean_pool , global_max_pool\n",
    "\n",
    "\n",
    "# class GCN_Graph_Max_Pool(torch.nn.Module):\n",
    "#     def __init__(self, hidden_dim, output_dim, num_layers, dropout):\n",
    "#         super(GCN_Graph_Max_Pool, self).__init__()\n",
    "\n",
    "        \n",
    "#         self.node_encoder = AtomEncoder(hidden_dim)\n",
    "\n",
    "       \n",
    "#         self.gnn_node = GCN(hidden_dim, hidden_dim,\n",
    "#             hidden_dim, num_layers, dropout, return_embeds=True)\n",
    "\n",
    "#         self.pool = global_max_pool\n",
    "\n",
    "#         self.linear = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "\n",
    "#     def reset_parameters(self):\n",
    "#       self.gnn_node.reset_parameters()\n",
    "#       self.linear.reset_parameters()\n",
    "\n",
    "#     def forward(self, batched_data):\n",
    "       \n",
    "#         x, edge_index, batch = batched_data.x, batched_data.edge_index, batched_data.batch\n",
    "#         embed = self.node_encoder(x)\n",
    "\n",
    "#         out = self.gnn_node(embed , edge_index)\n",
    "#         out = self.pool(out , batch)\n",
    "\n",
    "       \n",
    "#         out = self.linear(out)\n",
    "       \n",
    "\n",
    "#         return out\n",
    "    \n",
    "# model = GCN_Graph_Max_Pool(args['hidden_dim'],\n",
    "#               dataset.num_tasks, args['num_layers'],\n",
    "#               args['dropout']).to(device)\n",
    " \n",
    "# evaluator = Evaluator(name='ogbg-molhiv')\n",
    "# model.reset_parameters()\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
    "# loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# best_model = None\n",
    "# best_valid_acc = 0\n",
    "\n",
    "# for epoch in range(1, 1 + args[\"epochs\"]):\n",
    "#   print('Training...')\n",
    "#   loss = train(model, device, train_loader, optimizer, loss_fn)\n",
    "\n",
    "#   print('Evaluating...')\n",
    "#   train_result = eval(model, device, train_loader, evaluator)\n",
    "#   val_result = eval(model, device, valid_loader, evaluator)\n",
    "#   test_result = eval(model, device, test_loader, evaluator)\n",
    "\n",
    "#   train_acc, valid_acc, test_acc = train_result[dataset.eval_metric], val_result[dataset.eval_metric], test_result[dataset.eval_metric]\n",
    "#   if valid_acc > best_valid_acc:\n",
    "#       best_valid_acc = valid_acc\n",
    "#       best_model = copy.deepcopy(model)\n",
    "#   print(f'Epoch: {epoch:02d}, '\n",
    "#         f'Loss: {loss:.4f}, '\n",
    "#         f'Train: {100 * train_acc:.2f}%, '\n",
    "#         f'Valid: {100 * valid_acc:.2f}% '\n",
    "#         f'Test: {100 * test_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_acc = eval(best_model, device, train_loader, evaluator)[dataset.eval_metric]\n",
    "# valid_acc = eval(best_model, device, valid_loader, evaluator, save_model_results=True, save_file=\"valid_global_max_pool\")[dataset.eval_metric]\n",
    "# test_acc  = eval(best_model, device, test_loader, evaluator, save_model_results=True, save_file=\"test_global_max_pool\")[dataset.eval_metric]\n",
    "\n",
    "# print(f'Best model: '\n",
    "#     f'Train: {100 * train_acc:.2f}%, '\n",
    "#     f'Valid: {100 * valid_acc:.2f}% '\n",
    "#     f'Test: {100 * test_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7JXsMTBgeOI"
   },
   "source": [
    "# Submission\n",
    "\n",
    "You will need to submit four files on Gradescope to complete this notebook. \n",
    "\n",
    "1.   Your completed *XCS224W_Colab2.ipynb*. From the \"File\" menu select \"Download .ipynb\" to save a local copy of your completed Colab. \n",
    "2.  *ogbn-arxiv_node.csv* \n",
    "3.  *ogbg-molhiv_graph_valid.csv*\n",
    "4.  *ogbg-molhiv_graph_test.csv*\n",
    "\n",
    "Download the csv files by selecting the *Folder* icon on the left panel. \n",
    "\n",
    "To submit your work, zip the files downloaded in steps 1-4 above and submit to gradescope. **NOTE:** DO NOT rename any of the downloaded files. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ZGKqVEbbMEzf",
    "rwKbzhHUAckZ",
    "u05Z14TRYPGn",
    "gBi_t8n0iZ4P",
    "e7JXsMTBgeOI"
   ],
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
